{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17e1bcda-21f2-4d37-80c7-2b0401525a78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Enables autoreload; learn more at https://docs.databricks.com/en/files/workspace-modules.html#autoreload-for-python-modules\n",
    "# To disable autoreload; run %autoreload 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edbf8386-571c-48a9-b08c-549918e9d5b3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install dependancies"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydantic<3,>=2.7 in /databricks/python3/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (2.8.2)\n",
      "Collecting openai>=1.80 (from -r requirements.txt (line 2))\n",
      "  Downloading openai-1.92.2-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting aiolimiter>=1.2 (from -r requirements.txt (line 3))\n",
      "  Downloading aiolimiter-1.2.1-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting langchain-core>=0.3.1 (from -r requirements.txt (line 5))\n",
      "  Downloading langchain_core-0.3.66-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain-community>=0.3.1 (from -r requirements.txt (line 6))\n",
      "  Downloading langchain_community-0.3.26-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting neo4j>=5.20 (from -r requirements.txt (line 8))\n",
      "  Downloading neo4j-5.28.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting graspologic>=3.4 (from -r requirements.txt (line 9))\n",
      "  Downloading graspologic-3.4.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting networkx>=3.3 (from -r requirements.txt (line 10))\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting pyvis (from -r requirements.txt (line 11))\n",
      "  Downloading pyvis-0.3.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting llama-index-core>=0.12.0 (from -r requirements.txt (line 13))\n",
      "  Downloading llama_index_core-0.12.44-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting llama-index-graph-stores-neo4j>=0.4.0 (from -r requirements.txt (line 15))\n",
      "  Downloading llama_index_graph_stores_neo4j-0.4.6-py3-none-any.whl.metadata (694 bytes)\n",
      "Collecting llama-index-graph-stores-nebula>=0.4.0 (from -r requirements.txt (line 16))\n",
      "  Downloading llama_index_graph_stores_nebula-0.4.2-py3-none-any.whl.metadata (724 bytes)\n",
      "Collecting llama-index-llms-openai>=0.4.0 (from -r requirements.txt (line 18))\n",
      "  Downloading llama_index_llms_openai-0.4.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting llama-index-llms-azure-openai>=0.3.0 (from -r requirements.txt (line 19))\n",
      "  Downloading llama_index_llms_azure_openai-0.3.4-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting llama-index-llms-databricks>=0.3.0 (from -r requirements.txt (line 20))\n",
      "  Downloading llama_index_llms_databricks-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting llama-index-embeddings-huggingface>=0.5.0 (from -r requirements.txt (line 21))\n",
      "  Downloading llama_index_embeddings_huggingface-0.5.5-py3-none-any.whl.metadata (458 bytes)\n",
      "Collecting llama-index-embeddings-openai>=0.3.0 (from -r requirements.txt (line 22))\n",
      "  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
      "Collecting llama-index-embeddings-azure-openai>=0.3.0 (from -r requirements.txt (line 23))\n",
      "  Downloading llama_index_embeddings_azure_openai-0.3.8-py3-none-any.whl.metadata (503 bytes)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /databricks/python3/lib/python3.12/site-packages (from pydantic<3,>=2.7->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /databricks/python3/lib/python3.12/site-packages (from pydantic<3,>=2.7->-r requirements.txt (line 1)) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /databricks/python3/lib/python3.12/site-packages (from pydantic<3,>=2.7->-r requirements.txt (line 1)) (4.11.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /databricks/python3/lib/python3.12/site-packages (from openai>=1.80->-r requirements.txt (line 2)) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.80->-r requirements.txt (line 2)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /databricks/python3/lib/python3.12/site-packages (from openai>=1.80->-r requirements.txt (line 2)) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /databricks/python3/lib/python3.12/site-packages (from openai>=1.80->-r requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: sniffio in /databricks/python3/lib/python3.12/site-packages (from openai>=1.80->-r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /databricks/python3/lib/python3.12/site-packages (from openai>=1.80->-r requirements.txt (line 2)) (4.66.4)\n",
      "Collecting langsmith>=0.3.45 (from langchain-core>=0.3.1->-r requirements.txt (line 5))\n",
      "  Downloading langsmith-0.4.2-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /databricks/python3/lib/python3.12/site-packages (from langchain-core>=0.3.1->-r requirements.txt (line 5)) (8.2.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /databricks/python3/lib/python3.12/site-packages (from langchain-core>=0.3.1->-r requirements.txt (line 5)) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /databricks/python3/lib/python3.12/site-packages (from langchain-core>=0.3.1->-r requirements.txt (line 5)) (6.0.1)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /databricks/python3/lib/python3.12/site-packages (from langchain-core>=0.3.1->-r requirements.txt (line 5)) (24.2)\n",
      "Collecting langchain<1.0.0,>=0.3.26 (from langchain-community>=0.3.1->-r requirements.txt (line 6))\n",
      "  Downloading langchain-0.3.26-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /databricks/python3/lib/python3.12/site-packages (from langchain-community>=0.3.1->-r requirements.txt (line 6)) (2.0.30)\n",
      "Requirement already satisfied: requests<3,>=2 in /databricks/python3/lib/python3.12/site-packages (from langchain-community>=0.3.1->-r requirements.txt (line 6)) (2.32.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /databricks/python3/lib/python3.12/site-packages (from langchain-community>=0.3.1->-r requirements.txt (line 6)) (3.9.5)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community>=0.3.1->-r requirements.txt (line 6))\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community>=0.3.1->-r requirements.txt (line 6))\n",
      "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community>=0.3.1->-r requirements.txt (line 6))\n",
      "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /databricks/python3/lib/python3.12/site-packages (from langchain-community>=0.3.1->-r requirements.txt (line 6)) (1.26.4)\n",
      "Requirement already satisfied: pytz in /databricks/python3/lib/python3.12/site-packages (from neo4j>=5.20->-r requirements.txt (line 8)) (2024.1)\n",
      "Collecting POT<0.10,>=0.9 (from graspologic>=3.4->-r requirements.txt (line 9))\n",
      "  Downloading POT-0.9.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\n",
      "Collecting anytree<3.0.0,>=2.12.1 (from graspologic>=3.4->-r requirements.txt (line 9))\n",
      "  Downloading anytree-2.13.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting beartype<0.19.0,>=0.18.5 (from graspologic>=3.4->-r requirements.txt (line 9))\n",
      "  Downloading beartype-0.18.5-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting gensim<5.0.0,>=4.3.2 (from graspologic>=3.4->-r requirements.txt (line 9))\n",
      "  Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting graspologic-native<2.0.0,>=1.2.1 (from graspologic>=3.4->-r requirements.txt (line 9))\n",
      "  Downloading graspologic_native-1.2.5-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.6 kB)\n",
      "Collecting hyppo<0.5.0,>=0.4.0 (from graspologic>=3.4->-r requirements.txt (line 9))\n",
      "  Downloading hyppo-0.4.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: joblib<2.0.0,>=1.4.2 in /databricks/python3/lib/python3.12/site-packages (from graspologic>=3.4->-r requirements.txt (line 9)) (1.4.2)\n",
      "Requirement already satisfied: matplotlib<4.0.0,>=3.8.4 in /databricks/python3/lib/python3.12/site-packages (from graspologic>=3.4->-r requirements.txt (line 9)) (3.8.4)\n",
      "Requirement already satisfied: scikit-learn<2.0.0,>=1.4.2 in /databricks/python3/lib/python3.12/site-packages (from graspologic>=3.4->-r requirements.txt (line 9)) (1.4.2)\n",
      "Collecting scipy==1.12.0 (from graspologic>=3.4->-r requirements.txt (line 9))\n",
      "  Downloading scipy-1.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/60.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: seaborn<0.14.0,>=0.13.2 in /databricks/python3/lib/python3.12/site-packages (from graspologic>=3.4->-r requirements.txt (line 9)) (0.13.2)\n",
      "Requirement already satisfied: statsmodels<0.15.0,>=0.14.2 in /databricks/python3/lib/python3.12/site-packages (from graspologic>=3.4->-r requirements.txt (line 9)) (0.14.2)\n",
      "Collecting umap-learn<0.6.0,>=0.5.6 (from graspologic>=3.4->-r requirements.txt (line 9))\n",
      "  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: ipython>=5.3.0 in /databricks/python3/lib/python3.12/site-packages (from pyvis->-r requirements.txt (line 11)) (8.25.0)\n",
      "Requirement already satisfied: jinja2>=2.9.6 in /databricks/python3/lib/python3.12/site-packages (from pyvis->-r requirements.txt (line 11)) (3.1.4)\n",
      "Collecting jsonpickle>=1.4.1 (from pyvis->-r requirements.txt (line 11))\n",
      "  Downloading jsonpickle-4.1.1-py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting aiosqlite (from llama-index-core>=0.12.0->-r requirements.txt (line 13))\n",
      "  Downloading aiosqlite-0.21.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting banks<3,>=2.0.0 (from llama-index-core>=0.12.0->-r requirements.txt (line 13))\n",
      "  Downloading banks-2.1.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /databricks/python3/lib/python3.12/site-packages (from llama-index-core>=0.12.0->-r requirements.txt (line 13)) (1.2.14)\n",
      "Collecting dirtyjson<2,>=1.0.8 (from llama-index-core>=0.12.0->-r requirements.txt (line 13))\n",
      "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting filetype<2,>=1.2.0 (from llama-index-core>=0.12.0->-r requirements.txt (line 13))\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /databricks/python3/lib/python3.12/site-packages (from llama-index-core>=0.12.0->-r requirements.txt (line 13)) (2023.5.0)\n",
      "Collecting llama-index-workflows<2,>=1.0.1 (from llama-index-core>=0.12.0->-r requirements.txt (line 13))\n",
      "  Downloading llama_index_workflows-1.0.1-py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /databricks/python3/lib/python3.12/site-packages (from llama-index-core>=0.12.0->-r requirements.txt (line 13)) (1.6.0)\n",
      "Collecting nltk>3.8.1 (from llama-index-core>=0.12.0->-r requirements.txt (line 13))\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /databricks/python3/lib/python3.12/site-packages (from llama-index-core>=0.12.0->-r requirements.txt (line 13)) (10.3.0)\n",
      "Collecting setuptools>=80.9.0 (from llama-index-core>=0.12.0->-r requirements.txt (line 13))\n",
      "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /databricks/python3/lib/python3.12/site-packages (from llama-index-core>=0.12.0->-r requirements.txt (line 13)) (0.7.0)\n",
      "Collecting typing-inspect>=0.8.0 (from llama-index-core>=0.12.0->-r requirements.txt (line 13))\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: wrapt in /databricks/python3/lib/python3.12/site-packages (from llama-index-core>=0.12.0->-r requirements.txt (line 13)) (1.14.1)\n",
      "Collecting nebula3-python<4.0.0,>=3.8.0 (from llama-index-graph-stores-nebula>=0.4.0->-r requirements.txt (line 16))\n",
      "  Downloading nebula3_python-3.8.3-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: azure-identity<2,>=1.15.0 in /databricks/python3/lib/python3.12/site-packages (from llama-index-llms-azure-openai>=0.3.0->-r requirements.txt (line 19)) (1.19.0)\n",
      "Collecting llama-index-llms-openai-like<0.5,>=0.4.0 (from llama-index-llms-databricks>=0.3.0->-r requirements.txt (line 20))\n",
      "  Downloading llama_index_llms_openai_like-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.0 in /databricks/python3/lib/python3.12/site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface>=0.5.0->-r requirements.txt (line 21)) (0.24.5)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.1 in /databricks/python3/lib/python3.12/site-packages (from llama-index-embeddings-huggingface>=0.5.0->-r requirements.txt (line 21)) (3.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /databricks/python3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.3.1->-r requirements.txt (line 6)) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /databricks/python3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.3.1->-r requirements.txt (line 6)) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /databricks/python3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.3.1->-r requirements.txt (line 6)) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /databricks/python3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.3.1->-r requirements.txt (line 6)) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /databricks/python3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.3.1->-r requirements.txt (line 6)) (1.9.3)\n",
      "Requirement already satisfied: idna>=2.8 in /databricks/python3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai>=1.80->-r requirements.txt (line 2)) (3.7)\n",
      "Requirement already satisfied: azure-core>=1.31.0 in /databricks/python3/lib/python3.12/site-packages (from azure-identity<2,>=1.15.0->llama-index-llms-azure-openai>=0.3.0->-r requirements.txt (line 19)) (1.32.0)\n",
      "Requirement already satisfied: cryptography>=2.5 in /databricks/python3/lib/python3.12/site-packages (from azure-identity<2,>=1.15.0->llama-index-llms-azure-openai>=0.3.0->-r requirements.txt (line 19)) (42.0.5)\n",
      "Requirement already satisfied: msal>=1.30.0 in /databricks/python3/lib/python3.12/site-packages (from azure-identity<2,>=1.15.0->llama-index-llms-azure-openai>=0.3.0->-r requirements.txt (line 19)) (1.31.0)\n",
      "Requirement already satisfied: msal-extensions>=1.2.0 in /databricks/python3/lib/python3.12/site-packages (from azure-identity<2,>=1.15.0->llama-index-llms-azure-openai>=0.3.0->-r requirements.txt (line 19)) (1.2.0)\n",
      "Collecting griffe (from banks<3,>=2.0.0->llama-index-core>=0.12.0->-r requirements.txt (line 13))\n",
      "  Downloading griffe-1.7.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: platformdirs in /databricks/python3/lib/python3.12/site-packages (from banks<3,>=2.0.0->llama-index-core>=0.12.0->-r requirements.txt (line 13)) (3.10.0)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community>=0.3.1->-r requirements.txt (line 6))\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /databricks/python3/lib/python3.12/site-packages (from gensim<5.0.0,>=4.3.2->graspologic>=3.4->-r requirements.txt (line 9)) (5.2.1)\n",
      "Requirement already satisfied: certifi in /databricks/python3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai>=1.80->-r requirements.txt (line 2)) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in /databricks/python3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai>=1.80->-r requirements.txt (line 2)) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /databricks/python3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.80->-r requirements.txt (line 2)) (0.14.0)\n",
      "Requirement already satisfied: filelock in /databricks/python3/lib/python3.12/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface>=0.5.0->-r requirements.txt (line 21)) (3.13.1)\n",
      "Collecting minijinja>=1.0 (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface>=0.5.0->-r requirements.txt (line 21))\n",
      "  Downloading minijinja-2.10.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: numba>=0.46 in /databricks/python3/lib/python3.12/site-packages (from hyppo<0.5.0,>=0.4.0->graspologic>=3.4->-r requirements.txt (line 9)) (0.59.1)\n",
      "Collecting autograd>=1.3 (from hyppo<0.5.0,>=0.4.0->graspologic>=3.4->-r requirements.txt (line 9))\n",
      "  Downloading autograd-1.8.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: decorator in /databricks/python3/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis->-r requirements.txt (line 11)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /databricks/python3/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis->-r requirements.txt (line 11)) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /databricks/python3/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis->-r requirements.txt (line 11)) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /databricks/python3/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis->-r requirements.txt (line 11)) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /databricks/python3/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis->-r requirements.txt (line 11)) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /databricks/python3/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis->-r requirements.txt (line 11)) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /databricks/python3/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis->-r requirements.txt (line 11)) (5.14.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /databricks/python3/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis->-r requirements.txt (line 11)) (4.8.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.12/site-packages (from jinja2>=2.9.6->pyvis->-r requirements.txt (line 11)) (2.1.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /databricks/python3/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.3.1->-r requirements.txt (line 5)) (3.0.0)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain<1.0.0,>=0.3.26->langchain-community>=0.3.1->-r requirements.txt (line 6))\n",
      "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /databricks/python3/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core>=0.3.1->-r requirements.txt (line 5)) (3.10.11)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /databricks/python3/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core>=0.3.1->-r requirements.txt (line 5)) (1.0.0)\n",
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith>=0.3.45->langchain-core>=0.3.1->-r requirements.txt (line 5))\n",
      "  Downloading zstandard-0.23.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: transformers<5,>=4.37.0 in /databricks/python3/lib/python3.12/site-packages (from llama-index-llms-openai-like<0.5,>=0.4.0->llama-index-llms-databricks>=0.3.0->-r requirements.txt (line 20)) (4.44.0)\n",
      "Collecting llama-index-instrumentation>=0.1.0 (from llama-index-workflows<2,>=1.0.1->llama-index-core>=0.12.0->-r requirements.txt (line 13))\n",
      "  Downloading llama_index_instrumentation-0.2.0-py3-none-any.whl.metadata (252 bytes)\n",
      "Collecting pydantic<3,>=2.7 (from -r requirements.txt (line 1))\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/68.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.0/68.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic-core==2.33.2 (from pydantic<3,>=2.7->-r requirements.txt (line 1))\n",
      "  Downloading pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-extensions!=4.7.0,>=4.6.0 (from pydantic-core==2.20.1->pydantic<3,>=2.7->-r requirements.txt (line 1))\n",
      "  Downloading typing_extensions-4.14.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3,>=2.7->-r requirements.txt (line 1))\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4.0.0,>=3.8.4->graspologic>=3.4->-r requirements.txt (line 9)) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4.0.0,>=3.8.4->graspologic>=3.4->-r requirements.txt (line 9)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4.0.0,>=3.8.4->graspologic>=3.4->-r requirements.txt (line 9)) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4.0.0,>=3.8.4->graspologic>=3.4->-r requirements.txt (line 9)) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4.0.0,>=3.8.4->graspologic>=3.4->-r requirements.txt (line 9)) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4.0.0,>=3.8.4->graspologic>=3.4->-r requirements.txt (line 9)) (2.9.0.post0)\n",
      "Requirement already satisfied: future>=0.18.0 in /databricks/python3/lib/python3.12/site-packages (from nebula3-python<4.0.0,>=3.8.0->llama-index-graph-stores-nebula>=0.4.0->-r requirements.txt (line 16)) (0.18.3)\n",
      "Requirement already satisfied: httplib2>=0.20.0 in /usr/lib/python3/dist-packages (from nebula3-python<4.0.0,>=3.8.0->llama-index-graph-stores-nebula>=0.4.0->-r requirements.txt (line 16)) (0.20.4)\n",
      "Requirement already satisfied: six>=1.16.0 in /usr/lib/python3/dist-packages (from nebula3-python<4.0.0,>=3.8.0->llama-index-graph-stores-nebula>=0.4.0->-r requirements.txt (line 16)) (1.16.0)\n",
      "Requirement already satisfied: click in /databricks/python3/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core>=0.12.0->-r requirements.txt (line 13)) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /databricks/python3/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core>=0.12.0->-r requirements.txt (line 13)) (2023.10.3)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community>=0.3.1->-r requirements.txt (line 6))\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests<3,>=2->langchain-community>=0.3.1->-r requirements.txt (line 6)) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.12/site-packages (from requests<3,>=2->langchain-community>=0.3.1->-r requirements.txt (line 6)) (1.26.16)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn<2.0.0,>=1.4.2->graspologic>=3.4->-r requirements.txt (line 9)) (2.2.0)\n",
      "Requirement already satisfied: pandas>=1.2 in /databricks/python3/lib/python3.12/site-packages (from seaborn<0.14.0,>=0.13.2->graspologic>=3.4->-r requirements.txt (line 9)) (1.5.3)\n",
      "Requirement already satisfied: torch>=1.11.0 in /databricks/python3/lib/python3.12/site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface>=0.5.0->-r requirements.txt (line 21)) (2.4.0+cu124)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /databricks/python3/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain-community>=0.3.1->-r requirements.txt (line 6)) (3.0.1)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /databricks/python3/lib/python3.12/site-packages (from statsmodels<0.15.0,>=0.14.2->graspologic>=3.4->-r requirements.txt (line 9)) (0.5.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /databricks/python3/lib/python3.12/site-packages (from typing-inspect>=0.8.0->llama-index-core>=0.12.0->-r requirements.txt (line 13)) (1.0.0)\n",
      "Collecting pynndescent>=0.5 (from umap-learn<0.6.0,>=0.5.6->graspologic>=3.4->-r requirements.txt (line 9))\n",
      "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.12/site-packages (from cryptography>=2.5->azure-identity<2,>=1.15.0->llama-index-llms-azure-openai>=0.3.0->-r requirements.txt (line 19)) (1.16.0)\n",
      "Collecting h2<5,>=3 (from ht\n",
      "\n",
      "*** WARNING: max output size exceeded, skipping output. ***\n",
      "\n",
      "m \u001b[32m15.6/37.8 MB\u001b[0m \u001b[31m109.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/37.8 MB\u001b[0m \u001b[31m110.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.3/37.8 MB\u001b[0m \u001b[31m111.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m27.5/37.8 MB\u001b[0m \u001b[31m115.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m31.4/37.8 MB\u001b[0m \u001b[31m114.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m34.9/37.8 MB\u001b[0m \u001b[31m108.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m37.8/37.8 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m37.8/37.8 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m37.8/37.8 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m37.8/37.8 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m37.8/37.8 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m37.8/37.8 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m37.8/37.8 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m37.8/37.8 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m37.8/37.8 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m37.8/37.8 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m37.8/37.8 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m37.8/37.8 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m37.8/37.8 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m37.8/37.8 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m37.8/37.8 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m37.8/37.8 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m37.8/37.8 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m37.8/37.8 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m37.8/37.8 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m37.8/37.8 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m37.8/37.8 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m37.8/37.8 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m37.8/37.8 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m37.8/37.8 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m37.8/37.8 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.8/37.8 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m126.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyvis-0.3.2-py3-none-any.whl (756 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/756.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llama_index_core-0.12.44-py3-none-any.whl (7.6 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/7.6 MB\u001b[0m \u001b[31m122.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m113.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m113.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m113.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m113.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llama_index_graph_stores_neo4j-0.4.6-py3-none-any.whl (17 kB)\n",
      "Downloading llama_index_graph_stores_nebula-0.4.2-py3-none-any.whl (18 kB)\n",
      "Downloading llama_index_llms_openai-0.4.7-py3-none-any.whl (25 kB)\n",
      "Downloading llama_index_llms_azure_openai-0.3.4-py3-none-any.whl (7.3 kB)\n",
      "Downloading llama_index_llms_databricks-0.3.2-py3-none-any.whl (3.6 kB)\n",
      "Downloading llama_index_embeddings_huggingface-0.5.5-py3-none-any.whl (8.9 kB)\n",
      "Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading llama_index_embeddings_azure_openai-0.3.8-py3-none-any.whl (4.4 kB)\n",
      "Downloading anytree-2.13.0-py3-none-any.whl (45 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/45.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading banks-2.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading beartype-0.18.5-py3-none-any.whl (917 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/917.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m917.8/917.8 kB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/26.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/26.6 MB\u001b[0m \u001b[31m109.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/26.6 MB\u001b[0m \u001b[31m104.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/26.6 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/26.6 MB\u001b[0m \u001b[31m96.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/26.6 MB\u001b[0m \u001b[31m118.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m20.1/26.6 MB\u001b[0m \u001b[31m99.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m23.7/26.6 MB\u001b[0m \u001b[31m96.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading graspologic_native-1.2.5-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (364 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/364.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m364.6/364.6 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
      "Downloading hyppo-0.4.0-py3-none-any.whl (146 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/146.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.6/146.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jsonpickle-4.1.1-py3-none-any.whl (47 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/47.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain-0.3.26-py3-none-any.whl (1.0 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langsmith-0.4.2-py3-none-any.whl (367 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/367.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.7/367.7 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llama_index_llms_openai_like-0.4.0-py3-none-any.whl (4.6 kB)\n",
      "Downloading llama_index_workflows-1.0.1-py3-none-any.whl (36 kB)\n",
      "Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/444.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m109.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nebula3_python-3.8.3-py3-none-any.whl (331 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/331.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.3/331.3 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading POT-0.9.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (901 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/901.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m901.1/901.7 kB\u001b[0m \u001b[31m137.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m901.1/901.7 kB\u001b[0m \u001b[31m137.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m901.7/901.7 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/45.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.14.0-py3-none-any.whl (43 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/88.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.8/88.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiosqlite-0.21.0-py3-none-any.whl (15 kB)\n",
      "Downloading autograd-1.8.0-py3-none-any.whl (51 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Downloading llama_index_instrumentation-0.2.0-py3-none-any.whl (14 kB)\n",
      "Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading minijinja-2.10.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/56.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading zstandard-0.23.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/5.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m4.1/5.4 MB\u001b[0m \u001b[31m124.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m119.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading griffe-1.7.3-py3-none-any.whl (129 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading h2-4.2.0-py3-none-any.whl (60 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hpack-4.1.0-py3-none-any.whl (34 kB)\n",
      "Downloading hyperframe-6.1.0-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: filetype, dirtyjson, zstandard, typing-extensions, setuptools, scipy, python-dotenv, nltk, networkx, neo4j, minijinja, marshmallow, jsonpickle, hyperframe, httpx-sse, hpack, graspologic-native, deprecated, colorama, beartype, autograd, anytree, aiolimiter, typing-inspection, typing-inspect, pydantic-core, POT, h2, griffe, gensim, aiosqlite, pynndescent, pydantic, hyppo, dataclasses-json, umap-learn, pyvis, pydantic-settings, openai, nebula3-python, llama-index-instrumentation, langsmith, banks, llama-index-workflows, langchain-core, graspologic, llama-index-core, langchain-text-splitters, llama-index-llms-openai, llama-index-graph-stores-neo4j, llama-index-graph-stores-nebula, llama-index-embeddings-openai, llama-index-embeddings-huggingface, langchain, llama-index-llms-openai-like, llama-index-llms-azure-openai, langchain-community, llama-index-llms-databricks, llama-index-embeddings-azure-openai\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.11.0\n",
      "    Not uninstalling typing-extensions at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-59cd0d52-0eee-4843-b32d-956c86c44457\n",
      "    Can't uninstall 'typing_extensions'. No files were found to uninstall.\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 74.0.0\n",
      "    Not uninstalling setuptools at /usr/local/lib/python3.12/dist-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-59cd0d52-0eee-4843-b32d-956c86c44457\n",
      "    Can't uninstall 'setuptools'. No files were found to uninstall.\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.13.1\n",
      "    Not uninstalling scipy at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-59cd0d52-0eee-4843-b32d-956c86c44457\n",
      "    Can't uninstall 'scipy'. No files were found to uninstall.\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.8.1\n",
      "    Not uninstalling nltk at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-59cd0d52-0eee-4843-b32d-956c86c44457\n",
      "    Can't uninstall 'nltk'. No files were found to uninstall.\n",
      "  Attempting uninstall: networkx\n",
      "    Found existing installation: networkx 3.2.1\n",
      "    Not uninstalling networkx at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-59cd0d52-0eee-4843-b32d-956c86c44457\n",
      "    Can't uninstall 'networkx'. No files were found to uninstall.\n",
      "  Attempting uninstall: deprecated\n",
      "    Found existing installation: Deprecated 1.2.14\n",
      "    Not uninstalling deprecated at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-59cd0d52-0eee-4843-b32d-956c86c44457\n",
      "    Can't uninstall 'Deprecated'. No files were found to uninstall.\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.20.1\n",
      "    Not uninstalling pydantic-core at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-59cd0d52-0eee-4843-b32d-956c86c44457\n",
      "    Can't uninstall 'pydantic_core'. No files were found to uninstall.\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.8.2\n",
      "    Not uninstalling pydantic at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-59cd0d52-0eee-4843-b32d-956c86c44457\n",
      "    Can't uninstall 'pydantic'. No files were found to uninstall.\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.40.2\n",
      "    Not uninstalling openai at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-59cd0d52-0eee-4843-b32d-956c86c44457\n",
      "    Can't uninstall 'openai'. No files were found to uninstall.\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.1.133\n",
      "    Not uninstalling langsmith at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-59cd0d52-0eee-4843-b32d-956c86c44457\n",
      "    Can't uninstall 'langsmith'. No files were found to uninstall.\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.2.43\n",
      "    Not uninstalling langchain-core at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-59cd0d52-0eee-4843-b32d-956c86c44457\n",
      "    Can't uninstall 'langchain-core'. No files were found to uninstall.\n",
      "  Attempting uninstall: langchain-text-splitters\n",
      "    Found existing installation: langchain-text-splitters 0.2.4\n",
      "    Not uninstalling langchain-text-splitters at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-59cd0d52-0eee-4843-b32d-956c86c44457\n",
      "    Can't uninstall 'langchain-text-splitters'. No files were found to uninstall.\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.2.12\n",
      "    Not uninstalling langchain at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-59cd0d52-0eee-4843-b32d-956c86c44457\n",
      "    Can't uninstall 'langchain'. No files were found to uninstall.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "optuna 3.6.1 requires alembic>=1.5.0, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed POT-0.9.5 aiolimiter-1.2.1 aiosqlite-0.21.0 anytree-2.13.0 autograd-1.8.0 banks-2.1.2 beartype-0.18.5 colorama-0.4.6 dataclasses-json-0.6.7 deprecated-1.2.18 dirtyjson-1.0.8 filetype-1.2.0 gensim-4.3.3 graspologic-3.4.1 graspologic-native-1.2.5 griffe-1.7.3 h2-4.2.0 hpack-4.1.0 httpx-sse-0.4.1 hyperframe-6.1.0 hyppo-0.4.0 jsonpickle-4.1.1 langchain-0.3.26 langchain-community-0.3.26 langchain-core-0.3.66 langchain-text-splitters-0.3.8 langsmith-0.4.2 llama-index-core-0.12.44 llama-index-embeddings-azure-openai-0.3.8 llama-index-embeddings-huggingface-0.5.5 llama-index-embeddings-openai-0.3.1 llama-index-graph-stores-nebula-0.4.2 llama-index-graph-stores-neo4j-0.4.6 llama-index-instrumentation-0.2.0 llama-index-llms-azure-openai-0.3.4 llama-index-llms-databricks-0.3.2 llama-index-llms-openai-0.4.7 llama-index-llms-openai-like-0.4.0 llama-index-workflows-1.0.1 marshmallow-3.26.1 minijinja-2.10.2 nebula3-python-3.8.3 neo4j-5.28.1 networkx-3.5 nltk-3.9.1 openai-1.92.2 pydantic-2.11.7 pydantic-core-2.33.2 pydantic-settings-2.10.1 pynndescent-0.5.13 python-dotenv-1.1.1 pyvis-0.3.2 scipy-1.12.0 setuptools-80.9.0 typing-extensions-4.14.0 typing-inspect-0.9.0 typing-inspection-0.4.1 umap-learn-0.5.7 zstandard-0.23.0\n",
      "\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f538c1f3-59f0-4653-848f-1b29cd859535",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import modules"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 22:33:16.994407: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750977197.013531    1784 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750977197.019859    1784 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-26 22:33:17.040775: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from constants import * \n",
    "import networkx as nx\n",
    "import pyvis\n",
    "\n",
    "from src.helper_functions import *\n",
    "from src.llama_index_graph_rag_extractor import GraphRAGExtractor\n",
    "from src.graph_rag_store import GraphRAGStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4829773-1254-4139-a5cf-847142a2dbae",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configuring OpenAI params"
    }
   },
   "outputs": [],
   "source": [
    "# Manage environment variables required to attach LLM service in this context\n",
    "exec(open('azure_envars.py').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a123906d-4d1e-4a44-b1f7-923db4c19f7f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load and chunk parsed data"
    }
   },
   "outputs": [],
   "source": [
    "from main import create_graph_index\n",
    "save_idx_path = os.path.join(os.getcwd(), \"outputs/palantir_8k_10k_graph\")\n",
    "save_kg_path = os.path.join(os.getcwd(), \"outputs/8k_10k_kg.html\")\n",
    "\n",
    "index = create_graph_index(docs_dir= \"data\",\n",
    "                           # num_nodes = 1,\n",
    "                           save_index = True,\n",
    "                           save_kg = True,\n",
    "                           index_path = save_idx_path,\n",
    "                           kg_path = save_kg_path\n",
    "                        )\n",
    "# index.property_graph_store.build_communities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f96a8259-8b9e-44c4-8766-1a8f1408b93e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>predicate</th>\n",
       "      <th>object</th>\n",
       "      <th>filename</th>\n",
       "      <th>company</th>\n",
       "      <th>document_type</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>period</th>\n",
       "      <th>relationship_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Palantir Technologies Inc.</td>\n",
       "      <td>Issues</td>\n",
       "      <td>Class A Common Stock</td>\n",
       "      <td>2024 FY.pdf</td>\n",
       "      <td>Palantir Technologies</td>\n",
       "      <td>10K report</td>\n",
       "      <td>2024</td>\n",
       "      <td>Yearly</td>\n",
       "      <td>Palantir Technologies Inc. issues Class A Comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Class A Common Stock</td>\n",
       "      <td>Traded under symbol</td>\n",
       "      <td>PLTR</td>\n",
       "      <td>2024 FY.pdf</td>\n",
       "      <td>Palantir Technologies</td>\n",
       "      <td>10K report</td>\n",
       "      <td>2024</td>\n",
       "      <td>Yearly</td>\n",
       "      <td>Palantir's Class A Common Stock is traded unde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PLTR</td>\n",
       "      <td>Listed on</td>\n",
       "      <td>The Nasdaq Stock Market LLC</td>\n",
       "      <td>2024 FY.pdf</td>\n",
       "      <td>Palantir Technologies</td>\n",
       "      <td>10K report</td>\n",
       "      <td>2024</td>\n",
       "      <td>Yearly</td>\n",
       "      <td>The PLTR ticker is listed on The Nasdaq Stock ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Palantir Technologies Inc.</td>\n",
       "      <td>Files</td>\n",
       "      <td>10K report</td>\n",
       "      <td>2024 FY.pdf</td>\n",
       "      <td>Palantir Technologies</td>\n",
       "      <td>10K report</td>\n",
       "      <td>2024</td>\n",
       "      <td>Yearly</td>\n",
       "      <td>Palantir Technologies Inc. files an annual 10K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10K report</td>\n",
       "      <td>Submitted to</td>\n",
       "      <td>United States Securities and Exchange Commission</td>\n",
       "      <td>2024 FY.pdf</td>\n",
       "      <td>Palantir Technologies</td>\n",
       "      <td>10K report</td>\n",
       "      <td>2024</td>\n",
       "      <td>Yearly</td>\n",
       "      <td>The 10K report is submitted to the SEC as requ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4994</th>\n",
       "      <td>Twelfth Amendment</td>\n",
       "      <td>Amends</td>\n",
       "      <td>Credit Agreement</td>\n",
       "      <td>31_03_2022.pdf</td>\n",
       "      <td>Palantir Technologies</td>\n",
       "      <td>8K report</td>\n",
       "      <td>31-03-2022</td>\n",
       "      <td></td>\n",
       "      <td>The Twelfth Amendment modifies the terms of th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>Exhibit 10.1</td>\n",
       "      <td>Contains</td>\n",
       "      <td>Twelfth Amendment</td>\n",
       "      <td>31_03_2022.pdf</td>\n",
       "      <td>Palantir Technologies</td>\n",
       "      <td>8K report</td>\n",
       "      <td>31-03-2022</td>\n",
       "      <td></td>\n",
       "      <td>Exhibit 10.1 includes the full text of the Twe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>Palantir Technologies</td>\n",
       "      <td>No Outstanding Debt</td>\n",
       "      <td>Outstanding Amounts</td>\n",
       "      <td>31_03_2022.pdf</td>\n",
       "      <td>Palantir Technologies</td>\n",
       "      <td>8K report</td>\n",
       "      <td>31-03-2022</td>\n",
       "      <td></td>\n",
       "      <td>As of the report date, Palantir Technologies h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>Alexander C. Karp</td>\n",
       "      <td>Executive Officer</td>\n",
       "      <td>Palantir Technologies</td>\n",
       "      <td>31_03_2022.pdf</td>\n",
       "      <td>Palantir Technologies</td>\n",
       "      <td>8K report</td>\n",
       "      <td>31-03-2022</td>\n",
       "      <td></td>\n",
       "      <td>Alexander C. Karp is the Chief Executive Offic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>Morgan Stanley Senior Funding, Inc.</td>\n",
       "      <td>Administrative Agent</td>\n",
       "      <td>Credit Agreement</td>\n",
       "      <td>31_03_2022.pdf</td>\n",
       "      <td>Palantir Technologies</td>\n",
       "      <td>8K report</td>\n",
       "      <td>31-03-2022</td>\n",
       "      <td></td>\n",
       "      <td>Morgan Stanley Senior Funding, Inc. acts as th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4999 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  subject  ...                           relationship_description\n",
       "0              Palantir Technologies Inc.  ...  Palantir Technologies Inc. issues Class A Comm...\n",
       "1                    Class A Common Stock  ...  Palantir's Class A Common Stock is traded unde...\n",
       "2                                    PLTR  ...  The PLTR ticker is listed on The Nasdaq Stock ...\n",
       "3              Palantir Technologies Inc.  ...  Palantir Technologies Inc. files an annual 10K...\n",
       "4                              10K report  ...  The 10K report is submitted to the SEC as requ...\n",
       "...                                   ...  ...                                                ...\n",
       "4994                    Twelfth Amendment  ...  The Twelfth Amendment modifies the terms of th...\n",
       "4995                         Exhibit 10.1  ...  Exhibit 10.1 includes the full text of the Twe...\n",
       "4996                Palantir Technologies  ...  As of the report date, Palantir Technologies h...\n",
       "4997                    Alexander C. Karp  ...  Alexander C. Karp is the Chief Executive Offic...\n",
       "4998  Morgan Stanley Senior Funding, Inc.  ...  Morgan Stanley Senior Funding, Inc. acts as th...\n",
       "\n",
       "[4999 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.helper_functions import kg_relations_to_df\n",
    "kg_relations_to_df(index, save_as_csv = True, filename = \"outputs/triplet_relations/8k_10k_kg_relations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f90c7be-d094-4591-b47e-3fc0028f3f7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from azure.identity import ClientSecretCredential, DefaultAzureCredential, get_bearer_token_provider  \n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\")\n",
    "llm = SafeAzureOpenAI(\n",
    "    model= MODEL_NAME,\n",
    "    deployment_name= DEPLOYMENT_NAME,\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    azure_ad_token_provider= token_provider,\n",
    "    use_azure_ad= True,\n",
    "    api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
    "    temperature=0,\n",
    ")\n",
    "Settings.embed_model = embed_model\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46d61128-064f-46cb-99de-a82d05345221",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load index"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading llama_index.core.storage.kvstore.simple_kvstore from palantir_10k_graph/docstore.json.\n",
      "Loading llama_index.core.storage.kvstore.simple_kvstore from palantir_10k_graph/index_store.json.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import load_index_from_storage, StorageContext\n",
    "old_index = load_index_from_storage(storage_context=StorageContext.from_defaults(persist_dir=\"palantir_10k_graph\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6ea851a-1183-4584-b614-cfff3edc1a4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "old_index = PropertyGraphIndex.from_existing(\n",
    "    property_graph_store= index.property_graph_store  ,\n",
    "    # optional, neo4j also supports vectors directly\n",
    "    vector_store= index.vector_store,\n",
    "    embed_kg_nodes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d0f5212-5125-4056-8732-ebfd87e89bbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from src.custom_query_engine import GraphRAGQueryEngine \n",
    "\n",
    "graph_store = index.property_graph_store\n",
    "query_engine = GraphRAGQueryEngine(graph_store= graph_store,\n",
    "                                   llm=llm,\n",
    "                                   index=old_index\n",
    "                                )\n",
    "graph_store.build_communities()\n",
    "\n",
    "query_str = \"What was Palantir's revenue in 2022?\"\n",
    "response = query_engine.query(query_str)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bf9463c-6e76-4514-b49f-d63cebd389c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='10940b15-1320-4346-9b28-5c4229112bb8', embedding=None, metadata={'company': 'Palantir Technologies', 'document_type': 'Financial report', 'year': '2022', 'period': 'FY'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='2022 FY', node_type='4', metadata={'company': 'Palantir Technologies', 'document_type': 'Financial report', 'year': '2022', 'period': 'FY'}, hash='0df669ed3f4792903f9dd2eec2a8d3218fcc6b2b19f26987e255d6f686545d74'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='aa5da375-da1d-4ec6-b91f-c8b8e803366b', node_type='1', metadata={'company': 'Palantir Technologies', 'document_type': 'Financial report', 'year': '2022', 'period': 'FY'}, hash='aaea765350213aaa14cd7a06c06d7c9a15547f29661bb43621b79e098caa6bcc'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='20ee1a95-4dd0-4bec-b521-ad203cb0594c', node_type='1', metadata={}, hash='f5564e6c8ac2af616a75b19b0dfdcecd54133fde1f062a2245ace2ae104462d0')}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"Here are some facts extracted from the provided text:\\n\\n$1.9 billion Revenue (2022) -> Government Segment -> 56% Attributed to\\n\\nEach platform is comprised of user-facing applications that are targeted to the specific industries and sectors in which they are used.\\n\\nDespite  their  differences,  Gotham  and  Foundry  both  serve  as  central  operating  systems  for  our  customers. Where  they  vary  in  specific  functionality,  they  align  in approach. Both platforms, backed by Apollo, can be deployed in almost any environment.\\n\\nSimilarly, customers can now use Apollo to enable continuous deployment, configuration management, and central software operations management across almost any environment for their own software products.\\n\\n## Gotham\\n\\nGotham  enables  users  to  identify  patterns  hidden  deep  within  datasets,  ranging  from  signals  intelligence  sources  to  reports  from  confidential  informants.  It  also facilitates the hand-off between analysts and operational users, helping operators plan and execute real-world responses to threats that have been identified within the platform. Gotham is now used broadly across government functions.\\n\\nWe also offer Gotham to our commercial customers, including to those in the financial services industry in connection with fraud investigations.\\n\\n## Foundry\\n\\nFoundry transforms the ways organizations operate by creating a central operating system for their data. Individual users can integrate and analyze the data they need in one place. The speed with which users can experiment and test new ideas is what makes the software stick.\\n\\nData projects often fail because the steps and methods used to build data pipelines are difficult to understand and recreate. We built Foundry's backend to solve the root of this problem. The platform's graphical interface does the rest, allowing users to track and trace their pipelines so they know what the rows and columns in their tables represent and why they are there. All of our commercial customers now use it, as do several of our government customers.\\n\\n## Apollo\\n\\nWe have always prioritized meeting our customers wherever they need us most. We originally built Apollo to enable the continuous delivery of our software wherever our customers are: in the cloud, on-premises, or even more rugged environments. Today, Apollo enables the rapid, secure delivery of our software and updates across our business.\\n\\nIn 2021, we began offering Apollo as a commercial solution to allow our customers to securely deploy their own software in virtually any environment. Apollo provides a single control layer to coordinate ongoing delivery of new features, security updates, and platform configurations.\\n\\n## Our Customers\\n\\nWe work with many of the world's leading government and commercial institutions. As of December 31, 2022, we had 367 customers.\\n\\nAn overview of those customers and the ways in which they use our software follows below.\\n\\n## Table of Contents\\n\\n## Overview\\n\\nOur software is currently used across more than 60 industries around the world.\\n\\nOf the $1.9 billion in revenue that we generated in 2022, 56% came from customers in the government segment, and 44% came from customers in the commercial segment.\\n\\nOur business continues to have a global presence. In 2022, we earned 61% of our revenue from customers in the United States, and 39% from those abroad.\\n\\nThe average revenue for our top twenty customers during the trailing twelve months ended December 31, 2022 was $49.4 million, and is up from 2021, when the average revenue from our top 20 customers during the trailing twelve months ended December 31, 2021 was $43.6 million, demonstrating our expanding relationships with existing customers.\\n\\n## Our Software at Work\\n\\nOur software is in the hands of factory workers, soldiers, clinicians, prosecutors, investigators, claims adjusters, technicians, intelligence analysts, and social workers around  the  world.  It  is  used  in  a  variety  of  applications  including  by  utility  operations  analysts,  automotive  manufacturing  workers,  oil  and  gas  technicians  and operators, and pharmaceutical researchers in the United States; supply-chain managers in South Korea; assembly workers in France; public health administrators in the United Kingdom and the United States; and special forces personnel and military officials in the United States and abroad.\\n\\n## Industries and Sectors\\n\\nIn addition to supporting individual institutions, our platforms have become central operating systems for entire industries and sectors.\\n\\nOur work with Airbus S.A.S. ('Airbus'), for example, was initially focused on the production of the A350 aircraft. The deployment of our software soon grew into Skywise, our aviation platform that has become the central operating system of the airline industry.\\n\\nSkywise connects more than 10,000 aircraft across a growing community of airlines, maintenance and repair organizations, authorities, and Airbus. Decision makers at each of these institutions use Skywise to more efficiently design, manufacture, service, operate, and maintain their global fleets.\", mimetype='text/plain', start_char_idx=19049, end_char_idx=24044, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.8237512832146352)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/databricks.mlflow.trace": "[\"tr-e719d79487a5415aa31a3d3c3c5d3706\", \"tr-5396ada6523e4d89a5a6fdeb68540ea5\"]",
      "text/plain": [
       "[Trace(request_id=tr-e719d79487a5415aa31a3d3c3c5d3706), Trace(request_id=tr-5396ada6523e4d89a5a6fdeb68540ea5)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nodes = index.as_retriever(\n",
    "            similarity_top_k= 8\n",
    "        ).retrieve(query_str)\n",
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a96e073-4e0b-4449-94d0-f027fa285c61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class CustomRetriever(BaseRetriever):\n",
    "    \"\"\"Custom retriever that performs both semantic search and hybrid search.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_retriever: VectorIndexRetriever,\n",
    "        keyword_retriever: KeywordTableSimpleRetriever,\n",
    "        mode: str = \"AND\",\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "\n",
    "        self._vector_retriever = vector_retriever\n",
    "        self._keyword_retriever = keyword_retriever\n",
    "        if mode not in (\"AND\", \"OR\"):\n",
    "            raise ValueError(\"Invalid mode.\")\n",
    "        self._mode = mode\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "\n",
    "        vector_nodes = self._vector_retriever.retrieve(query_bundle)\n",
    "        keyword_nodes = self._keyword_retriever.retrieve(query_bundle)\n",
    "\n",
    "        vector_ids = {n.node.node_id for n in vector_nodes}\n",
    "        keyword_ids = {n.node.node_id for n in keyword_nodes}\n",
    "\n",
    "        combined_dict = {n.node.node_id: n for n in vector_nodes}\n",
    "        combined_dict.update({n.node.node_id: n for n in keyword_nodes})\n",
    "\n",
    "        if self._mode == \"AND\":\n",
    "            retrieve_ids = vector_ids.intersection(keyword_ids)\n",
    "        else:\n",
    "            retrieve_ids = vector_ids.union(keyword_ids)\n",
    "\n",
    "        retrieve_nodes = [combined_dict[rid] for rid in retrieve_ids]\n",
    "        return retrieve_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "672108de-6aa5-4f05-bf68-dba3797496c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from llama_index.core.indices.property_graph import LLMSynonymRetriever\n",
    "from typing import List, Tuple\n",
    "\n",
    "prompt = (\n",
    "    \"Given some initial query, generate synonyms or related keywords up to {max_keywords} in total, \"\n",
    "    \"considering possible cases of capitalization, pluralization, common expressions, etc.\\n\"\n",
    "    \"Provide all synonyms/keywords separated by '^' symbols: 'keyword1^keyword2^...'\\n\"\n",
    "    \"Note, result should be in one-line, separated by '^' symbols.\"\n",
    "    \"----\\n\"\n",
    "    \"QUERY: {query_str}\\n\"\n",
    "    \"----\\n\"\n",
    "    \"KEYWORDS: \"\n",
    ")\n",
    "\n",
    "\n",
    "def parse_fn(self, output: str) -> list[str]:\n",
    "    matches = output.strip().split(\"^\")\n",
    "\n",
    "    # capitalize to normalize with ingestion\n",
    "    return [x.strip().capitalize() for x in matches if x.strip()]\n",
    "\n",
    "def parse_fn1(response_str: str) -> List[Tuple[str, str, str]]:\n",
    "    lines = response_str.split(\"\\n\")\n",
    "    triples = [line.split(\",\") for line in lines]\n",
    "    return triples\n",
    "\n",
    "synonym_retriever = LLMSynonymRetriever(\n",
    "    index.property_graph_store,\n",
    "    llm= llm,\n",
    "    # include source chunk text with retrieved paths\n",
    "    include_text=False,\n",
    "    synonym_prompt=prompt,\n",
    "    # output_parsing_fn= parse_fn1,\n",
    "    max_keywords=10,\n",
    "    # the depth of relations to follow after node retrieval\n",
    "    path_depth=1,\n",
    ")\n",
    "\n",
    "retriever = index.as_retriever(sub_retrievers= [synonym_retriever])\n",
    "retriever.retrieve(query_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dc6fa57-92e9-4868-9442-f03cd67cecd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Neo4j Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e0b6c16-3ca3-4c96-8108-1aeb0fcbb789",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from llama_index.graph_stores.neo4j import Neo4jPropertyGraphStore\n",
    "from llama_index.core import PropertyGraphIndex\n",
    "\n",
    "graph_store = Neo4jPropertyGraphStore(\n",
    "    username=\"neo4j\",\n",
    "    password=\" \",\n",
    "    url=\" \",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79368930-a098-46b8-9dc4-fb5e031aaf81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "uri = \" \"\n",
    "username = \"neo4j\"\n",
    "password = \" \"\n",
    "driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "\n",
    "def create_graph(tx, subject, predicate, object_):\n",
    "    query = f\"\"\"\n",
    "    MERGE (s:Entity {{name: $subject}})\n",
    "    MERGE (o:Entity {{name: $object}})\n",
    "    MERGE (s)-[r:{predicate.upper().replace(\" \", \"_\")}]->(o)\n",
    "    \"\"\"\n",
    "    tx.run(query, subject=subject, object=object_)\n",
    "\n",
    "triplets = index.property_graph_store.get_triplets()\n",
    "with driver.session() as session:\n",
    "    for s, p, o in triplets:\n",
    "        session.write_transaction(create_graph, s, p, o)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "full_pipeline",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
